# -*- coding: utf-8 -*-
"""sent folder model from csv.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HUrKIo9-sKU6ZDYp3lcWDTn2s0McQpFO
"""

import pandas as pd
import re
import spacy
import nltk
from spacy.lang.en import English
import random
from gensim import corpora
import pickle
import gensim
import pyLDAvis.gensim
nltk.download('wordnet')
from nltk.corpus import wordnet as wn


# df.shape

# scoped_df = df.iloc[-1000:]

# scoped_df.index

# scoped_df.to_excel('scoped sent test.xlsx', index=False)
# scoped_df.iloc[0]['Body']
# df.iloc[-1000:]['Body']
# documents.isna().value_counts()
# first_doc = documents[2:3].to_list()
# first_doc[0]

def remove_headers(email):
  scoped_email = email
  scoped_email = re.sub('From:(.*)\r\n',' ',scoped_email)
  scoped_email = re.sub('Sent:(.*)\r\n',' ',scoped_email)
  scoped_email = re.sub('Received:(.*)\r\n',' ',scoped_email)
  scoped_email = re.sub('To:(.*)\r\n',' ',scoped_email)
  scoped_email = re.sub('Cc:(.*)\r\n',' ',scoped_email)
  scoped_email = re.sub('Subject:',' ',scoped_email)
  return scoped_email

def remove_footers(email):
  scoped_email = email
  scoped_email = re.sub('This e-mail transmission contains confidential(.*)jurisdiction of the State of New York',
                        ' ',scoped_email)
  scoped_email = re.sub('This e-mail may contain confidential(.*)delete this e-mail immediately',
                        ' ',scoped_email)
  scoped_email = re.sub('This e-mail may also contain protected health information(.*)consent for release of this type of information.',
                        ' ',scoped_email)
  scoped_email = re.sub('This email \(including any attachments\) is intended(.*)delete the email and any attachments',
                        ' ',scoped_email)
  scoped_email = re.sub('Confidential Disclaimer(.*)destroy all copies of the original message', ' ', scoped_email)
  return scoped_email

def remove_sigs(email):
  scoped_email = email
  scoped_email = re.sub('<[^>]*>', ' ', scoped_email)
  scoped_email = re.sub('Claims Services Representative',' ',scoped_email)
  scoped_email = re.sub('Claims Services',' ',scoped_email)
  scoped_email = re.sub('Claims Administration',' ',scoped_email)
  scoped_email = re.sub('New York State Insurance Fund',' ',scoped_email)
  scoped_email = re.sub('199 Church Street, New York, NY 10007',' ',scoped_email)
  scoped_email = re.sub('thank',' ',scoped_email.lower())
  scoped_email = re.sub('thanks',' ',scoped_email.lower())
  scoped_email = re.sub('please',' ',scoped_email.lower())
  scoped_email = re.sub('facebook',' ',scoped_email.lower())
  scoped_email = re.sub('youtube',' ',scoped_email.lower())
  scoped_email = re.sub('twitter',' ',scoped_email.lower())
  scoped_email = re.sub('instagram',' ',scoped_email.lower())
  scoped_email = re.sub('youtube',' ',scoped_email.lower())
  scoped_email = re.sub('wdang@nysif.com',' ',scoped_email.lower())
  scoped_email = re.sub('212.312.7608',' ',scoped_email.lower())
  return scoped_email

def clean_email(email):
  return remove_sigs(remove_headers(remove_footers(email)))

# clean_email(first_doc[0])


# len(cleaned_emails)


def tokenize(text):
    lda_tokens = []
    parser = English()
    tokens = parser(text)
    for token in tokens:
        if token.orth_.isspace():
            continue
        elif token.like_url:
            lda_tokens.append('URL')
        elif token.orth_.startswith('@'):
            lda_tokens.append('SCREEN_NAME')
        else:
            lda_tokens.append(token.lower_)
    return lda_tokens


def get_lemma(word):
    lemma = wn.morphy(word)
    if lemma is None:
        return word
    else:
        return lemma
    

def get_lemma2(word):
    return WordNetLemmatizer().lemmatize(word)



def prepare_text_for_lda(text):
    en_stop = set(nltk.corpus.stopwords.words('english'))
    tokens = tokenize(text)
    tokens = [token for token in tokens if len(token) > 4]
    tokens = [token for token in tokens if token not in en_stop]
    tokens = [get_lemma(token) for token in tokens]
    return tokens

def create_tokens():
    text_data = []
    with open('body_only.csv') as f:
        for line in f:
            tokens = prepare_text_for_lda(line)
            # if random.random() > .99:
            if random.random() > .8:
                print(tokens)
                text_data.append(tokens)
    return text_data

def create_dictionary(text_data):
    dictionary = corpora.Dictionary(text_data)
    corpus = [dictionary.doc2bow(text) for text in text_data]
    pickle.dump(corpus, open('corpus.pkl', 'wb'))
    dictionary.save('dictionary.gensim')

def create_topics():
    NUM_TOPICS = 5
    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)
    ldamodel.save('model5.gensim')
    topics = ldamodel.print_topics(num_words=4)
    for topic in topics:
        print(topic)

# [email for email in cleaned_emails if 'following' in email][5:10]

def visualize_topics():
    # !pip install pyLDAvis
    dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')
    corpus = pickle.load(open('corpus.pkl', 'rb'))
    lda = gensim.models.ldamodel.LdaModel.load('model5.gensim')
    lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)
    pyLDAvis.display(lda_display)

def main():
    # df = pd.read_csv('test sent export.zip', encoding='iso-8859-1')
    df = pd.read_excel('scoped sent test.xlsx')
    documents = df.iloc[-1000:]['Body']
    spacy.load('en')
    nltk.download('wordnet')
    nltk.download('stopwords')
    cleaned_emails = [clean_email(document) for document in documents]
    pd.Series(cleaned_emails).to_csv('body_only.csv')
    create_dictionary(create_tokens())
    create_topics()
    visualize_topics()

